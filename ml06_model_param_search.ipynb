{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械学習特論 第6回課題 モデル・パラメータの探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google colab で実行する場合は、次の行の先頭の # を削除してこのブロックを実行する\n",
    "#!pip install japanize-matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion-MNISTデータセットをダウンロードして、実験用データを準備 (70000枚のうち7000枚を利用)\n",
    "def load_fashion_mnist_data():\n",
    "    data = fetch_openml('Fashion-MNIST')\n",
    "    _x = np.array(data['data'].astype(np.float32))\n",
    "    _y = np.array(data['target'].astype(np.int32))\n",
    "    _, x, _, y = train_test_split(_x, _y, test_size=0.1, random_state=1, stratify=_y) \n",
    "    return x, y\n",
    "\n",
    "# 一括処理のためにデータセットの辞書を作成\n",
    "dataset = {'fashon-mnist': load_fashion_mnist_data()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル・パラメータ探索のためにモデル・パラメータ探索範囲の辞書を作成\n",
    "model = {\n",
    "    # svm\n",
    "    'SVC': {'model': svm.SVC(),\n",
    "            'parameters': {'kernel': [\"linear\", \"rbf\"],\n",
    "                           'C': [0.1,1,10],}, \n",
    "           },\n",
    "    # ランダムフォレスト\n",
    "    'randomforest':{'model': RandomForestClassifier(random_state=1),\n",
    "                    'parameters': {'max_depth': [5,10,15],\n",
    "                                   'n_estimators': [50,100,150]}, \n",
    "                   },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## dataset:fashon-mnist  x_train:5250 x_test:1750 y_train:5250 y_test:1750\n",
      "model:SVC best parameters: {'C': 10, 'kernel': 'rbf'} accuracy_score: train_data: 0.9899 test_data: 0.87486\n",
      "model:randomforest best parameters: {'max_depth': 15, 'n_estimators': 100} accuracy_score: train_data: 0.99771 test_data: 0.844\n"
     ]
    }
   ],
   "source": [
    "# 辞書に格納したデータセット・モデルについて最良パラメータを GridSearchCV 探索して性能を確認\n",
    "for dataset_key in dataset.keys():\n",
    "    # データを学習用と検証用に分割\n",
    "    x, y = dataset[dataset_key]\n",
    "    x_train, x_test, y_train, y_test = \\\n",
    "        train_test_split(x, y, test_size=0.25, random_state=1) # 検証用データに25%を割当て\n",
    "    print(f'## dataset:{dataset_key} ',\n",
    "          f'x_train:{len(x_train)} x_test:{len(x_test)} y_train:{len(y_train)} y_test:{len(y_test)}')\n",
    "\n",
    "    # 辞書に格納したモデルそれぞれについて性能を測定\n",
    "    for model_key in model.keys():\n",
    "        # 学習用データを利用してモデル・パラメータを探索\n",
    "        clf = model[model_key]['model']\n",
    "        parameters = model[model_key]['parameters']\n",
    "        clf_search = GridSearchCV(clf, parameters)\n",
    "        clf_search.fit(x_train, np.array(y_train).ravel())\n",
    "\n",
    "        # 探索したモデルの性能(正答率)を学習用データと検証用データで評価\n",
    "        predict_train = clf_search.best_estimator_.predict(x_train)\n",
    "        train_score = metrics.accuracy_score(y_train, predict_train)\n",
    "        predict_test = clf_search.best_estimator_.predict(x_test)\n",
    "        test_score = metrics.accuracy_score(y_test, predict_test)\n",
    "        print(f'model:{model_key} best parameters:', clf_search.best_params_,\n",
    "              f'accuracy_score: train_data:{train_score: 0.5} test_data:{test_score: 0.5}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル・パラメータ探索のためにモデル・パラメータ探索範囲の辞書を作成\n",
    "from scipy import stats\n",
    "model = {\n",
    "    # svm\n",
    "    'SVC': {'model': svm.SVC(),\n",
    "            'parameters': {'kernel': [\"linear\", \"rbf\"],\n",
    "                           'C': stats.expon(scale=1),}, \n",
    "           },\n",
    "    # ランダムフォレスト\n",
    "    'randomforest':{'model': RandomForestClassifier(random_state=1),\n",
    "                    'parameters': {'max_depth': stats.randint(5,15),\n",
    "                                   'n_estimators': stats.randint(50,200)}, \n",
    "                   },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## dataset:fashon-mnist  x_train:5250 x_test:1750 y_train:5250 y_test:1750\n",
      "model:SVC best parameters: {'C': 3.866308691595505, 'kernel': 'rbf'} model:SVC accuracy_score: train_data: 0.95981 test_data: 0.87143\n",
      "model:randomforest best parameters: {'max_depth': 12, 'n_estimators': 160} model:randomforest accuracy_score: train_data: 0.98286 test_data: 0.84629\n"
     ]
    }
   ],
   "source": [
    "# 辞書に格納したデータセット・モデルについて最良パラメータを RandomizedSearchCV 探索して性能を確認\n",
    "for dataset_key in dataset.keys():\n",
    "    # データを学習用と検証用に分割\n",
    "    x, y = dataset[dataset_key]\n",
    "    x_train, x_test, y_train, y_test = \\\n",
    "        train_test_split(x, y, test_size=0.25, random_state=1) # 検証用データに25%を割当て\n",
    "    print(f'## dataset:{dataset_key} ',\n",
    "          f'x_train:{len(x_train)} x_test:{len(x_test)} y_train:{len(y_train)} y_test:{len(y_test)}')\n",
    "\n",
    "    # 辞書に格納したモデルそれぞれについて性能を測定\n",
    "    for model_key in model.keys():\n",
    "        # 学習用データを利用してモデル・パラメータを探索\n",
    "        clf = model[model_key]['model']\n",
    "        parameters = model[model_key]['parameters']\n",
    "        clf_search = RandomizedSearchCV(clf, parameters)\n",
    "        clf_search.fit(x_train, np.array(y_train).ravel())\n",
    "\n",
    "        # 探索したモデルの性能(正答率)を学習用データと検証用データで評価\n",
    "        predict_train = clf_search.best_estimator_.predict(x_train)\n",
    "        train_score = metrics.accuracy_score(y_train, predict_train)\n",
    "        predict_test = clf_search.best_estimator_.predict(x_test)\n",
    "        test_score = metrics.accuracy_score(y_test, predict_test)\n",
    "        print(f'model:{model_key} best parameters:', clf_search.best_params_,\n",
    "              f'model:{model_key} accuracy_score: train_data:{train_score: 0.5} test_data:{test_score: 0.5}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18ef0afcf35f1452430268c7ef685ac367525865953635d3b087fb3264879c09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
